{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from gensim.models import Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_vader_score(song):\n",
    "    ss = sid.polarity_scores(song)\n",
    "    return ss['compound']\n",
    "\n",
    "def get_tb_score(song):\n",
    "    polarity = TextBlob(song).sentiment.polarity\n",
    "    return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('', encoding='UTF-8')\n",
    "\n",
    "df.dropna(axis=0, how='any', subset=['Lyrics'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "add_stopwords = ['la', 'ay', 'oh', 'ohh', 'ooh', 'yeah', 'hey', 'im', 'like', 'get', 'go', 'na', 'got', 'duh', \n",
    "                 'ya', 'ah', 'let', 'cuz', 'wit', 'da', 'gonna', 'cause', 'imma', 'dem', 'dey', 'dats', 'wat',\n",
    "                 'dont', 'cant', 'wanna', 'see', 'make', 'want', 'youre', 'keep', 'lets', 'dat', 'ba', 'bah', 'aint',\n",
    "                 'aah', 'say', 'aye', 'come'\n",
    "                ]\n",
    "\n",
    "stopwords.extend(add_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(text, stops=False):\n",
    "    '''\n",
    "    Remove all unusual text patterns that are often found in song lyrics\n",
    "    '''\n",
    "    # lowercase and remove anything between brackets ([Chorus] for example)\n",
    "    text = text.lower()\n",
    "    text = re.sub(pattern='\\[.+?\\]()?', repl=' ', string=text)\n",
    "    \n",
    "    # Remove occurrences of (x2) or (x3) etc\n",
    "    text = re.sub(pattern='(\\()?x\\d+(\\))?', repl=' ', string=text)\n",
    "    \n",
    "    # Remove words with stretched out sounds ie. 'aaaaahhhh'\n",
    "    text = re.sub(pattern='(\\\\b\\\\w*?)(\\\\w)\\\\2{2,}(\\\\w*)', repl=' ', string=text)\n",
    "    \n",
    "    # Remove new lines\n",
    "    text = re.sub(pattern='^\\n', repl=' ', string=text)\n",
    "    \n",
    "    # Change rockin' to rocking, ie. \n",
    "    text = re.sub(pattern=\"n\\\\\\' \", repl='ng ', string=text)\n",
    "    \n",
    "    # Remove stopwords (default false)\n",
    "    if stops:\n",
    "        text = ' '.join([word for word in text.split(' ') if word not in stopwords])\n",
    "    \n",
    "    # Remove numbers & other non-alphabetical characters\n",
    "    text = re.sub(pattern='[^a-zA-Z ]', repl=' ', string=text)\n",
    "    \n",
    "    # Remove extra spaces (leading/trailing, doubles)\n",
    "    text = re.sub(pattern='( ){2,}', repl=' ', string=text)\n",
    "    text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'] = df['Song'] + '_' + df['Artist']\n",
    "\n",
    "df['Lyrics'] = df['Lyrics'].apply(lambda x: text_processing(x))\n",
    "\n",
    "df['Vader'] = np.nan\n",
    "\n",
    "df['TextBlob'] = np.nan\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_plot(sample, cluster_size):\n",
    "    sample_labels = gensim.models.doc2vec.TaggedDocument\n",
    "\n",
    "    content_train = []\n",
    "\n",
    "    j = 0\n",
    "\n",
    "    for song in sample.values:\n",
    "        content_train.append(sample_labels(song, [j]))\n",
    "        j += 1\n",
    "\n",
    "    print('Number of songs processed: ', j)\n",
    "    \n",
    "    d2v_model = Doc2Vec(content_train, vector_size=100, window=10, min_count=500, workers=7, dm=1, alpha=0.025, min_alpha=0.001)\n",
    "\n",
    "    d2v_model.train(content_train, total_examples=d2v_model.corpus_count, epochs=10, start_alpha=0.002, end_alpha=0.016)\n",
    "    \n",
    "    kmeans_model = KMeans(n_clusters=cluster_size, init='k-means++', max_iter=100)\n",
    "\n",
    "    X = kmeans_model.fit(d2v_model.docvecs.vectors_docs)\n",
    "\n",
    "    labels = kmeans_model.labels_.tolist()\n",
    "\n",
    "    l = kmeans_model.fit_predict(d2v_model.docvecs.vectors_docs)\n",
    "\n",
    "    pca = PCA(n_components=2).fit(d2v_model.docvecs.vectors_docs)\n",
    "\n",
    "    datapoint = pca.transform(d2v_model.docvecs.vectors_docs)\n",
    "    \n",
    "    plt.figure(figsize=(12,12))\n",
    "    \n",
    "    label1 = [\"#53868B\", \"#DB2929\"]\n",
    "\n",
    "    color = [label1[i] for i in labels]\n",
    "\n",
    "    plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "    centroids = kmeans_model.cluster_centers_\n",
    "    centroidpoint = pca.transform(centroids)\n",
    "    \n",
    "    plt.title('K-Means Cluster of Song Lyrics')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='x', s=150, c='#000000')\n",
    "    \n",
    "    plt.savefig('cluster.png')\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(df):\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for i in range(len(sixties_set['Song'])):\n",
    "        df.loc[i, ['Vader']] = get_vader_score(df['Lyrics'][i])\n",
    "        df.loc[i, ['TextBlob']] = get_tb_score(df['Lyrics'][i])\n",
    "\n",
    "    c1_v = df[df['Cluster'] == 0].mean(axis=0)['Vader']\n",
    "    c2_v =  df[df['Cluster'] == 1].mean(axis=0)['Vader']\n",
    "    \n",
    "    c1_tb = df[df['Cluster'] == 0].mean(axis=0)['TextBlob']\n",
    "    c2_tb =  df[df['Cluster'] == 1].mean(axis=0)['TextBlob']\n",
    "    \n",
    "    return c1_v, c2_v, c1_tb, c2_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'] = cluster_plot(df['Lyrics'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixties_set = df[df['Year'] < 1970]\n",
    "seventies_set = df[(df['Year'] >= 1970) & (df['Year'] < 1980)]\n",
    "eighties_set = df[(df['Year'] >= 1980) & (df['Year'] < 1990)]\n",
    "nineties_set = df[(df['Year'] >= 1990) & (df['Year'] < 2000)]\n",
    "millenial_set = df[(df['Year'] >= 2000) & (df['Year'] < 2010)]\n",
    "tens_set = df[df['Year'] >= 2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores.append(sentiment_scores(sixties_set))\n",
    "scores.append(sentiment_scores(seventies_set))\n",
    "scores.append(sentiment_scores(eighties_set))\n",
    "scores.append(sentiment_scores(nineties_set))\n",
    "scores.append(sentiment_scores(millenial_set))\n",
    "scores.append(sentiment_scores(tens_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_vscore = []\n",
    "c2_vscore = []\n",
    "c1_tbscore = []\n",
    "c2_tbscore = []\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    c1_vscore.append(scores[i][0])\n",
    "    c2_vscore.append(scores[i][1])\n",
    "    c1_tbscore.append(scores[i][2])\n",
    "    c2_tbscore.append(scores[i][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(range(1965, 2025, 10), c1_vscore);\n",
    "plt.plot(range(1965, 2025, 10), c2_vscore);\n",
    "plt.title('Mean Vader Sentiment Score Per Decade')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Vader Sentiment Score')\n",
    "plt.legend(labels=['Cluster 1', 'Cluster 2'], loc='lower left');\n",
    "plt.savefig('Vader.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "plt.plot(range(1965, 2025, 10), c1_tbscore);\n",
    "plt.plot(range(1965, 2025, 10), c2_tbscore);\n",
    "\n",
    "plt.title('Mean TextBlob Sentiment Score Per Decade')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('TextBlob Sentiment Score')\n",
    "plt.legend(labels=['Cluster 1', 'Cluster 2'], loc='lower left');\n",
    "plt.savefig('tb.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_tbscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
